{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:43.502096Z",
     "start_time": "2017-11-07T05:54:43.482383Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:44.553992Z",
     "start_time": "2017-11-07T05:54:43.810657Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:45.008370Z",
     "start_time": "2017-11-07T05:54:44.908365Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_train(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:50.163246Z",
     "start_time": "2017-11-07T05:54:46.854525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = ImageFolderDataset('./data/train_data/', transform=transform_train)\n",
    "valid_ds = ImageFolderDataset('./data/valid_data/', transform=transform_test)\n",
    "train_valid_ds = ImageFolderDataset('./data/train_valid/', transform=transform_train)\n",
    "test_ds = ImageFolderDataset('./data/TestSet/', transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:50.193538Z",
     "start_time": "2017-11-07T05:54:50.165522Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_ds, batch_size=64, shuffle=True, last_batch='keep')\n",
    "valid_data = DataLoader(valid_ds, batch_size=64, shuffle=True, last_batch='keep')\n",
    "train_valid_data = DataLoader(train_valid_ds, batch_size=64, shuffle=True, last_batch='keep')\n",
    "test_data = DataLoader(test_ds, batch_size=128, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T02:01:44.832944Z",
     "start_time": "2017-11-07T02:01:44.819364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:54:56.034918Z",
     "start_time": "2017-11-07T05:54:56.012160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from densenet import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T05:55:00.703319Z",
     "start_time": "2017-11-07T05:54:58.909598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "model.initialize(ctx=mx.gpu(1))\n",
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-07T02:01:44.630Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=True)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if epoch == 149 or epoch == 224:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            bs = data.shape[0]\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += get_acc(output, label)\n",
    "            total += bs\n",
    "        writer.add_scalars('loss', {'train': train_loss / len(train_data)}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            valid_loss = 0\n",
    "            for data, label in valid_data:\n",
    "                bs = data.shape[0]\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                valid_total += bs\n",
    "            valid_acc = valid_correct / valid_total\n",
    "            writer.add_scalars('loss', {'valid': valid_loss / len(valid_data)}, epoch)\n",
    "            writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train acc %f, Valid Loss: %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            correct / total, valid_loss / len(valid_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-07T02:01:45.763Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(1)\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_decay = 0.1\n",
    "train(model, train_valid_data, None, num_epochs, learning_rate,\n",
    "      weight_decay, ctx, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_params('./densenet.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T06:04:40.603322Z",
     "start_time": "2017-11-07T05:56:18.961866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preds = []\n",
    "for data, _ in test_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    output = model(data)\n",
    "    pred_label = output.argmax(1)\n",
    "    preds.extend(pred_label.astype(int).asnumpy())\n",
    "\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key = lambda x:str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_ds.synsets[x])\n",
    "df.to_csv('submission_densenet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
