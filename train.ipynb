{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sherlock/anaconda3/envs/mx/lib/python3.6/json/encoder.py:199: DeprecationWarning: Interpreting naive datetime as local 2017-10-25 21:45:12.491960. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行数据集的重新分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/trainLabels.csv', 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    tokens = [i.rstrip().split(',') for i in lines]\n",
    "    idx_label = dict((int(idx), label) for idx, label in tokens)\n",
    "labels = set(idx_label.values())\n",
    "\n",
    "num_train = len(os.listdir('./data/train/'))\n",
    "\n",
    "num_train_tuning = int(num_train * (1 - 0.1))\n",
    "\n",
    "num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "\n",
    "label_count = dict()\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "for train_file in os.listdir('./data/train/'):\n",
    "    idx = int(train_file.split('.')[0])\n",
    "    label = idx_label[idx]\n",
    "    mkdir_if_not_exist(['./data', 'train_valid', label])\n",
    "    shutil.copy(os.path.join('./data/train/', train_file),\n",
    "               os.path.join('./data/train_valid', label))\n",
    "    if label not in label_count or label_count[label] < num_train_tuning_per_label:\n",
    "        mkdir_if_not_exist(['./data/train_data', label])\n",
    "        shutil.copy(os.path.join('./data/train', train_file),\n",
    "                   os.path.join('./data/train_data', label))\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "    else:\n",
    "        mkdir_if_not_exist(['./data/valid_data', label])\n",
    "        shutil.copy(os.path.join('./data/train/', train_file),\n",
    "                   os.path.join('./data/valid_data', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data.vision import ImageFolderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolderDataset('./data/train_data/', transform=transform_train)\n",
    "valid_ds = ImageFolderDataset('./data/valid_data/', transform=transform_test)\n",
    "train_valid_ds = ImageFolderDataset('./data/train_valid/', transform=transform_train)\n",
    "test_ds = ImageFolderDataset('./data/testSet/', transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_ds, batch_size=16, shuffle=True, last_batch='keep')\n",
    "valid_data = DataLoader(valid_ds, batch_size=16, shuffle=True, last_batch='keep')\n",
    "train_valid_data = DataLoader(train_valid_ds, batch_size=64, shuffle=True, last_batch='keep')\n",
    "test_data = DataLoader(test_ds, batch_size=128, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=mx.init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.HybridBlock):\n",
    "    def __init__(self, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4 * growthRate\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv1 = nn.Conv2D(\n",
    "                interChannels,\n",
    "                kernel_size=1,\n",
    "                use_bias=False,\n",
    "                weight_initializer=init.Normal(math.sqrt(2. / interChannels)))\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(\n",
    "                growthRate,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                use_bias=False,\n",
    "                weight_initializer=init.Normal(\n",
    "                    math.sqrt(2. / (9 * growthRate))))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = F.concat(* [x, out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SingleLayer(nn.HybridBlock):\n",
    "    def __init__(self, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv1 = nn.Conv2D(\n",
    "                growthRate,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                use_bias=False,\n",
    "                weight_initializer=init.Normal(\n",
    "                    math.sqrt(2. / (9 * growthRate))))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.concat(* [x, out], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.HybridBlock):\n",
    "    def __init__(self, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv1 = nn.Conv2D(\n",
    "                nOutChannels,\n",
    "                kernel_size=1,\n",
    "                use_bias=False,\n",
    "                weight_initializer=init.Normal(math.sqrt(2. / nOutChannels)))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.Pooling(out, kernel=(2, 2), stride=(2, 2), pool_type='avg')\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.HybridBlock):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth - 4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2 * growthRate\n",
    "        with self.name_scope():\n",
    "            self.conv1 = nn.Conv2D(\n",
    "                nChannels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                use_bias=False,\n",
    "                weight_initializer=init.Normal(math.sqrt(2. / nChannels)))\n",
    "            self.dense1 = self._make_dense(growthRate, nDenseBlocks,\n",
    "                                           bottleneck)\n",
    "\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        with self.name_scope():\n",
    "            self.trans1 = Transition(nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        with self.name_scope():\n",
    "            self.dense2 = self._make_dense(growthRate, nDenseBlocks,\n",
    "                                           bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        with self.name_scope():\n",
    "            self.trans2 = Transition(nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        with self.name_scope():\n",
    "            self.dense3 = self._make_dense(growthRate, nDenseBlocks,\n",
    "                                           bottleneck)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.fc = nn.Dense(nClasses)\n",
    "\n",
    "    def _make_dense(self, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = nn.HybridSequential()\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.add(Bottleneck(growthRate))\n",
    "            else:\n",
    "                layers.add(SingleLayer(growthRate))\n",
    "        return layers\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = F.Pooling(\n",
    "            F.relu(self.bn1(out)),\n",
    "            global_pool=1,\n",
    "            pool_type='avg',\n",
    "            kernel=(8, 8))\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
    "                            bottleneck=True, nClasses=10)\n",
    "net.hybridize()\n",
    "net.initialize(ctx=mx.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=True)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if epoch == 149 or epoch == 224:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            bs = data.shape[0]\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += get_acc(output, label)\n",
    "            total += bs\n",
    "        writer.add_scalars('loss', {'train': train_loss / len(train_data)}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            valid_loss = 0\n",
    "            for data, label in valid_data:\n",
    "                bs = data.shape[0]\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                valid_total += bs\n",
    "            valid_acc = valid_correct / valid_total\n",
    "            writer.add_scalars('loss', {'valid': valid_loss / len(valid_data)}, epoch)\n",
    "            writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train acc %f, Valid Loss: %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            correct / total, valid_loss / len(valid_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 1.534016, Train acc 0.438511, Valid Loss: 1.067974, Valid acc 0.624200, Time 00:05:32, lr 0.1\n",
      "Epoch 1. Train Loss: 0.999840, Train acc 0.650444, Valid Loss: 0.848804, Valid acc 0.708000, Time 00:05:36, lr 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-edaa1a8ea779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybridize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train(net, train_data, valid_data, num_epochs, learning_rate,\n\u001b[0;32m----> 9\u001b[0;31m       weight_decay, ctx, lr_decay)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-953a4b797d2c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_decay)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda3/envs/mx/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda3/envs/mx/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ctx = mx.gpu(0)\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_decay = 0.1\n",
    "\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate,\n",
    "      weight_decay, ctx, lr_decay)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mx]",
   "language": "python",
   "name": "conda-env-mx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
